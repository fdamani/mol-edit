8.22
	we have two same models training on logp04 and qed.
		logp04: /tigress/fdamani/mol-edit-output/onmt-logp04/checkpoints/train_valid_share/model-mlpattention/model-brnnenc-rnndec-2layer-600wordembed-600embed-shareembedding-mlpattention-adamoptim \
		qed: /tigress/fdamani/mol-edit-output/onmt-qed/checkpoints/train_valid_360_share/model-brnnenc-rnndec-2layer-600wordembed-600embed-shareembedding-mlpattention-adamoptim_step_2000.pt
			- this needs to be filed under train_valid_share, not train_valid_360.

				etached from 110169.pts-84.tigergpu
				[detached from 138817.pts-27.tigergpu


	- is word vec size too big? try 128 instead of 600.


	- we need to fully understand the limits of seq2seq lstm with attention
		- major limitation on certain test sets is inability to generate samples with high TM similarity.
			- possible solutions:
				- different attention
				- smaller word embedding (128 instead of 500) + shared to prevent overfitting.
				- more layers, different regularization
				- different beam search method
			- to better understand this limtation, compare results to our own 5k validation set. how do they differ? what makes our test set different than theirs?
				- why do we perform much worse on theirs?
	- once we understand this limitation we can try more complex models
		- apply to potency data.
		- copy
		- multi-objective optimization.

















	TODO
		- will copying help increase TM similarity?
			- need to run same model but with copy to assess.


		- running a shared embedding model (brnn, 600 size, 2 layer) for qed with mlp attention
		- lets run an equivalent model for logp
		- show it can be used for multi-task optimization.

		- key to multi-task will be high TM similarity on test data.


		- run bigger model (already ready in train.sh)
			- does this improve performance on test set?

		- change beam search - use google nmt to prefer shorter sequences
		- copy mechanism
		- feed back in




		- get tensorsboard working to save plots of validation ppl
		- start decay earlier (before 50k). 
		- save all beam outputs
			- take the one that meets similarity.







	- follow paper:
		- train a qed model with 88k compounds
		- train a 2 layer lstm with attention, dropout, label smoothing for a very large number of iters



		- change translate code to report all beam compounds
			- change evaluate code to look at all K beams per src compound
				- report the best of the K beams
				- compute diversity of K beams
				- try out different penalizations on translate
					- shorter strings have higher probability of being compounds
		- run a few more models
			- is xent reported for training? can we get validation loss printed?
			- model is overfitting - how to fix?
				- more layers+dropout
				- label smoothing.
			- lets stick to lstm+attention for now. this should do well.
				- try different types of attention.

8.9
	- does the RL loss need to be normalized by number of tokens to avoid bias towards shorter sequence?
	- try out a different baseline (instead of greedy decoding)
	- right now we are performing one MC sample - should we be doing more?
	- try label smoothing to prevent model from being overconfident about predicting the target character?
8.8
	- problem with rl reward func
		- if sampled compound is invalid we should not return 0 loss
			- if 0 loss there is no gradient which means we are not learning to avoid this!
		- our reward should be negative so it learns to avoid in the future.
		- probability of valid decoded should go up. this is implicit in our function.

8.7

	- write batching code for decoding without teacher forcing
		- should be able to copy validate func with target lengths modified
			to when each sample samples an EOS token.
		- this should allow for fast mini-batching and application to RL.

	- in addition to mini-batching code, initialize with xent weights and direct
		rl training is failing. we need to gradually include it.
		- options: mix xent with rl loss
		- xent with rl loss on last step and work way backward.

8.1
	- take out accept-reject
	- start w/ simple reward - maximize qed while tanimoto similarity is >= .4
	- what does training do?



	- initialize RL policy with xent trained to convergence
	- experiment with different reward functions
		- first try to optimize QED/logp
		- see if we can change the weights such that it improves this metric (on training and valid)
			- plot before and after
		- no accept-reject sampling, just single sample (in many cases rw will be 0 if not valid molecule this is fine--if in general sample generation from policy results in high likelihood of being a sample you're good)
	- review different options on how to integrate RL training
		- iterative RL fine tuning
			- train w/ xent then optimize qed with rl
				- then optimize qed and TM sim with RL
				- then optimize qed, TM sim, and logp with RL.


7.31
	- implement RL training objective (batch size 1)
	- start with pretrained model (plot TM similarity)
	- do a few steps of gradient descent (plot TM similarity) does it improve?

7.30
	- train a model on one of the synthetic tasks (regular lstm) with larger hidden units
	- how to measure scaffold diversity?
	- how to encode this desiderata into model?
	- try encoding tanimoto similarity into model and optimizing for this metric
	- once we do this, we can do the same for scaffold diversity
	- then maybe a combo between the two.



	- train a model on potency data (simple seq2seq model)
	- given output, assess scaffold diversity and tanimoto similarity
	- how do we directly encode that desiderata into our model?


7.18
	- compute tanimoto sim histogram for training data.
	- implement attention
		- need to speed up attention
			- try static context, read about how others speed this up
			- truncated backpropagation through time?
			- bottleneck is backprop.
	- train a model with attention
	- collect tanimoto simlarity data.
	- train language model for unsupervised pre-training.




	- need to add better regularization to 1 layer model
	- implement attention
	- collect potency dataset (all pairs with tanimoto similarity > .4)
		- look at potency measurements -> what is the right transform? plot histograms of this data.
	- potency predictor
	- run on a more challenging dataset with fewer samples and higher tanimoto similarity constraint?
		- fewer samples means it might generalize poorer
		- higher similarity constraint might be harder to generate.
	- try out some RL ideas on maintaining that constraint.


7.17
	- running 1 layer and 2 layer models with saved checkpoints
		- dropout included for 2 layer model

	todo
		- debug beam search 
			- confirm it works
		- implement attention-based architecture.
		- implement multiple layered rnn with batchnorm.
		- write code to generate potency dataset.
			- all pairs with tanimoto similarity > .4
		- language model pre-training.

7.16
	TODO
		- add regularization to network
			- tried 2 layers + dropout
			- need more regularization
				- smaller batch size
				- look into other ways people regularize
					- better optimization schedule
		- add model checkpoints - every 5k save.
		- run a model for longer - 500000 epochs.
		- beam search
		- unsupervised pre-training (language model)
		- attention

7.15
	- plots of all metrics
	- training/validation loss
	- percent valid decoded
	- histogram of qed (target) and (input)
	- percent successful in target range.


	TODO
		- implement beam search to approximate arg max p(y|x)
		- implement attention and transformer models
			- do these help with tanimoto similarity? we think yes.
		- look at potency data again.
			- ask vishnu about the potency measurements
			- what range is -logp in. what values does it take on.
			- create multiple datasets
				- create one dataset with all pairs that have similarity
				> .4
					- from this dataset, we can derive stricter thresholds and limit based on potency measurements.
			- do this today it might take awhile.

		- other decoders
			- beam search
			- ones that encourage diversity
			- longer decodes?
		- strong regression model to predict potency.


7.14
	- implemented evalute_func using greedy sampling
		- delta qed
		- percent valid decoded
	TODO:
		- for QED task, compute percent of decoded compounds that are in the [.9, 1.0] range (conditioned on valid decoding)
		- percent decoded successfully.
		- implement alternative decoders
			- sampler
			- beam search
		- plots for everything.

7.12
	- validation metrics (use validation data)
		- teacher forcing at validation time to measure NLL.
		- compute delta QED for decoded samples use this as validation metric.
			- try a few different versions 
				- greedy sampling 
				- sample multiple times and average.

7.11
	- refactored seq2seq code
	- new alphabet class ("Lang")
	- new training code
	- new evaluation code
	- encoder and decoder (no attention) nets implemented
	- successfully overfitting to 1k training pairs.

	Next:
		- measure validation performance
		- scale to larger dataset
		- implement attention-based decoders
7.8.19
	TODO:
		- run regression model (smiles and selfies) for logp06 and qed and compare r^2 values
		- implement seq2seq model (see wengong jin implementation)
		- implement variational seq2seq model (see wengong jin implementation)
		- how do these compare to their work? compare on selfies representation.
		- generate a complete set of potency mmpa training datasets
			- varying similarity thresholds
			- varying levels of potency
			- write script to run this.

	DONE:
		- generated regression training data for penalized logp and qed


7.3.19
	TODO:
		- generate training data (view different versions-- moderate and high tanimoto simlarity, inactive to active, inactive to moderately active, inactive to highly active)
		- implement seq2seq model
		- generate suite of evaluation methods to evaluate results.



7.2.19
	- mini-batching using packed sequence.
	- SELFIES-based representation
		- 498 compounds can not be encoded into SELFIES (see mol-edit/output/pot_pred_selfies/)
		- r^2 score on validation is .75 while SMILES was ~.83.
			- SELFIES vocab size is 88 while SMILES is 38
				- current implementation does not do any additional regularization (besides GRU cell)
				- 2x number of parameters -> discrepancy might be explained by overfitting.
			- remove 498 invalid SELFIES compounds from SMILES dataset?
	- initial mmpa training data generation
		- compute tanimoto simlarity based on morgan fingerprint.
		- initial strategy to generate training data:
			- find pairs between inactive (potency = -3.) and active (potency != -3.) with high tanimoto similarity.
				- use rest of inactive compounds (235k) to form pairs. -> keep high tanimoto similarity.
7.1.19
	- overfit RNN to small dataset
	- run on full dataset and show validation / train loss
	- minor optimizations:
		- pass as input to forward full word and have self.rnn return final hidden state
		- mini-batching
6.26.19
	- implemented RNN for potency prediction
6.25.19
	- Predict potency conditioned on structure
		- Implement RNN
		- Compare SMILES and SELFIES based representations.
